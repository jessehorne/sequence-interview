Sequence Interview
===

Here is the documentation for the challenge associated with the Sequence Interview linked below.

https://hackmd.io/@0xsequence/rJu7LHxDR

---

# Overview

In this repository, I have created a very simple set of scripts to pull data, transform it and normalize it using BigQuery.

**Please note:** This is a very simple example of how to complete this challenge and it is sparse due to my goal to limit the challenge to only a couple of hours of work. Please see the "What I Would Do With More Time" section that details what I would do if I had more time to improve this project.

---

# Setup

The scripts in this project assume that you have the following...

1. A Google Drive CSV data source
2. A GCP Account, Project (need Project ID), and BigQuery Dataset

The Project ID should be `sequenceinterview` and the dataset should be named `transactions`.

3. A table in the dataset named "tx" with a schema as follows...

timestamp (datetime), event (string), project_id (integer), value(integer)

4. A table in the dataset named "tx_flat" with a schema as follows...

day (date), project_id (integer), total_transactions (integer), volume (integer)

5. A proper .env file (see .env.example for recreating it)

# Running

There is a specific run order for these scripts.

## Step 1. Fetch data from datasource

```go
go run cmd/pull_transactions_data.go
```

## Step 2. Transform and Upload to BigQuery (table: tx)

**NOTE:** The date below can/should be changed to what was generated by the command above. See the `./data` directory.

```go
go run cmd/transform_and_upload_to_bigquery.go data/2024.07.01.csv
```

## Step 3. Normalize and Merge in BigQuery Table (table: tx_flat)
```go
go run cmd/normalize_data_in_bigquery.go
```

When you have run those three scripts, you should see that `tx_flat` contains what's needed to complete the challenge.

---

# What I Would Do With More Time

## 1. Coin Gecko

In this example, you can see a function in `cmd/transform_and_upload_to_bigquery.go` named `getCoinValueInUSD`. I decided to not use Coin Gecko because after hassling with it for about 20-30 minutes, I kept running into rate limits and unexpected response from one of the history endpoints. I decided to implement a very simple function to randomly generate and cache USD values for currency symbols.

With more time, I would set up an efficient way (with a paid account for coin gecko) to get these histories and cache them so that I wouldn't need to make so many requests.

## 2. Terraform

In this example, I specified (for the most part) what exactly you need for this to run. The README is not ideal though and what I'd prefer to do is set up terraform to automagically provision and deploy these GCP resources, including containerizing the actual scripts, running them as a cloud function (or cloud run) and scheduling them with cloud scheduler. 

## 3. Better docs

The majority of scripts, structs and methods are not properly documented. I would want to get it to the level so that if documentation was generated it would be helpful.

## 4. Testing, Logging and Backups

As you can see, there is no unit/system/integration tests for the code and no sort of validation on data produced in BQ or pulled from the data source, no sort of logging to alert us when rows do not fulfil our needs and no staging env or backups really... just what we transform and move into BQ. I completed this challenge with the assumption that the google drive data source will always exist.

## 5. Code cleanup

The scripts have some repetitive code that could be fixed. I made each script assuming it would need to run completely separate of other scripts which was properly a poor assumption to make since they are so lightweight.

---

# THANKS!

## PLEASE LET ME KNOW WHEN TO DELETE THIS REPO! IT IS PUBLIC!